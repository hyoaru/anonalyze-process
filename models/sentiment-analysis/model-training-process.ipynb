{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Model Training\n",
    "Sentiment analysis model training using data set from kaggle consisting of **937,854 observations** of labeled text content from twitter\n",
    "<br>\n",
    "Kaggle url: [https://www.kaggle.com/datasets/tariqsays/sentiment-dataset-with-1-million-tweets](https://www.kaggle.com/datasets/tariqsays/sentiment-dataset-with-1-million-tweets)\n",
    "<br> <br>\n",
    "**Dataset attributes**\n",
    "* 0 - language used\n",
    "* 1 - text\n",
    "* 2 - label (positive, negative, uncertainty, litigious)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cabrera/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/cabrera/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/cabrera/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/cabrera/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/cabrera/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/cabrera/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/cabrera/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Language</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Charlie_Corley @Kristine1G @amyklobuchar @Sty...</td>\n",
       "      <td>en</td>\n",
       "      <td>litigious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#BadBunny: Como dos gotas de agua: Joven se di...</td>\n",
       "      <td>es</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://t.co/YJNiO0p1JV Flagstar Bank disclose...</td>\n",
       "      <td>en</td>\n",
       "      <td>litigious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rwanda is set to host the headquarters of Unit...</td>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OOPS. I typed her name incorrectly (today’s br...</td>\n",
       "      <td>en</td>\n",
       "      <td>litigious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937849</th>\n",
       "      <td>@Juice_Lemons in the dark. it’s so good</td>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937850</th>\n",
       "      <td>8.SSR &amp;amp; Disha Salian case should be solved...</td>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937851</th>\n",
       "      <td>*ACCIDENT:  Damage Only* - Raleigh Fire Depart...</td>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937852</th>\n",
       "      <td>@reblavoie So happy for her! She’s been incred...</td>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937853</th>\n",
       "      <td>I'm lost and I'm found but</td>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>937854 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Text Language      Label\n",
       "0       @Charlie_Corley @Kristine1G @amyklobuchar @Sty...       en  litigious\n",
       "1       #BadBunny: Como dos gotas de agua: Joven se di...       es   negative\n",
       "2       https://t.co/YJNiO0p1JV Flagstar Bank disclose...       en  litigious\n",
       "3       Rwanda is set to host the headquarters of Unit...       en   positive\n",
       "4       OOPS. I typed her name incorrectly (today’s br...       en  litigious\n",
       "...                                                   ...      ...        ...\n",
       "937849            @Juice_Lemons in the dark. it’s so good       en   positive\n",
       "937850  8.SSR &amp; Disha Salian case should be solved...       en   negative\n",
       "937851  *ACCIDENT:  Damage Only* - Raleigh Fire Depart...       en   negative\n",
       "937852  @reblavoie So happy for her! She’s been incred...       en   positive\n",
       "937853                         I'm lost and I'm found but       en   negative\n",
       "\n",
       "[937854 rows x 3 columns]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/dataset.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 937854 entries, 0 to 937853\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   Text      937854 non-null  object\n",
      " 1   Language  937831 non-null  object\n",
      " 2   Label     937854 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 21.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info(verbose = True, show_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the column names lowercase and underscore separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Text': 'text', 'Language': 'language', 'Label': 'label'}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names_replacement_map = {}\n",
    "for column in df.columns:\n",
    "  column_names_replacement_map[column] = column.strip().replace(' ', '_').lower()\n",
    "\n",
    "column_names_replacement_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Charlie_Corley @Kristine1G @amyklobuchar @Sty...</td>\n",
       "      <td>en</td>\n",
       "      <td>litigious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#BadBunny: Como dos gotas de agua: Joven se di...</td>\n",
       "      <td>es</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://t.co/YJNiO0p1JV Flagstar Bank disclose...</td>\n",
       "      <td>en</td>\n",
       "      <td>litigious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rwanda is set to host the headquarters of Unit...</td>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OOPS. I typed her name incorrectly (today’s br...</td>\n",
       "      <td>en</td>\n",
       "      <td>litigious</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text language      label\n",
       "0  @Charlie_Corley @Kristine1G @amyklobuchar @Sty...       en  litigious\n",
       "1  #BadBunny: Como dos gotas de agua: Joven se di...       es   negative\n",
       "2  https://t.co/YJNiO0p1JV Flagstar Bank disclose...       en  litigious\n",
       "3  Rwanda is set to host the headquarters of Unit...       en   positive\n",
       "4  OOPS. I typed her name incorrectly (today’s br...       en  litigious"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename(columns=column_names_replacement_map)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retaining only observations that are using the english language to stay within the scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['language'] == 'en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excluding the language field as it is no longer of need and to reduce memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[column for column in df.columns if column != 'language']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing observations that are tagged as litigious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['label'] != 'litigious']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the features into its proper data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].astype('str')\n",
    "df['label'] = df['label'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rwanda is set to host the headquarters of Unit...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It sucks for me since I'm focused on the natur...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@ShawnTarloff @itsmieu you can also relate thi...</td>\n",
       "      <td>uncertainty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Social Security. Constant political crises dis...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@FilmThePoliceLA A broken rib can puncture a l...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691243</th>\n",
       "      <td>@Juice_Lemons in the dark. it’s so good</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691244</th>\n",
       "      <td>8.SSR &amp;amp; Disha Salian case should be solved...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691245</th>\n",
       "      <td>*ACCIDENT:  Damage Only* - Raleigh Fire Depart...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691246</th>\n",
       "      <td>@reblavoie So happy for her! She’s been incred...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691247</th>\n",
       "      <td>I'm lost and I'm found but</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>691248 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text        label\n",
       "0       Rwanda is set to host the headquarters of Unit...     positive\n",
       "1       It sucks for me since I'm focused on the natur...     negative\n",
       "2       @ShawnTarloff @itsmieu you can also relate thi...  uncertainty\n",
       "3       Social Security. Constant political crises dis...     negative\n",
       "4       @FilmThePoliceLA A broken rib can puncture a l...     negative\n",
       "...                                                   ...          ...\n",
       "691243            @Juice_Lemons in the dark. it’s so good     positive\n",
       "691244  8.SSR &amp; Disha Salian case should be solved...     negative\n",
       "691245  *ACCIDENT:  Damage Only* - Raleigh Fire Depart...     negative\n",
       "691246  @reblavoie So happy for her! She’s been incred...     positive\n",
       "691247                         I'm lost and I'm found but     negative\n",
       "\n",
       "[691248 rows x 2 columns]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 691248 entries, 0 to 691247\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    691248 non-null  object\n",
      " 1   label   691248 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 10.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info(verbose = True, show_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking how much of the observations' sentiment are positive, negative, and uncertain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "positive       248516\n",
       "negative       244146\n",
       "uncertainty    198586\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "Making a data pipeline to:\n",
    "* Denoise: removing the twitter usernames and non-alphabetical characters and stripping it of white space\n",
    "* Stopwords removal: stripping out the stopwords in the content such as `[a, an, the, and, but, or]` to improve data quality\n",
    "* Lemmatization: reducing words to their base form e.g. `[changing, changed, change] -> change`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the username and non-alphabetical characters in the content to reduce noise and improve data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoiser(df: pd.DataFrame):\n",
    "  def strip(text: str):\n",
    "    text = re.sub(r'@\\w+', '', text) \n",
    "    text = re.sub(r'[^a-zA-Z ]', '', text)\n",
    "    text = re.sub(r'https\\w+', '', text)\n",
    "    text = re.sub(r'http\\w+', '', text)\n",
    "    text = text.strip()\n",
    "    return text.lower()\n",
    "\n",
    "  df['text'] = df['text'].apply(strip)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stopwords for standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_remover(df: pd.DataFrame):\n",
    "  matcher = re.compile(r\"|\".join([fr\"\\b{word}\\b\" for word in stopwords.words(\"english\")]))\n",
    "  def remove_stopwords(text: str):\n",
    "    return \" \".join(matcher.sub('', text).split())\n",
    "\n",
    "  df['text'] = df['text'].apply(remove_stopwords)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing observations with null text values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_content_observation_remover(df: pd.DataFrame):\n",
    "  df = df[~df['text'].isnull()]\n",
    "  df = df.reset_index(drop=True)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing words to their base or lemmatizing to enhance the effectiveness of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(df: pd.DataFrame):\n",
    "  wordnet_lemmatizer = WordNetLemmatizer()\n",
    "  tokenizer = WordPunctTokenizer()\n",
    "\n",
    "  wordnet_pos_tag_map = {\n",
    "    \"J\": wordnet.ADJ,\n",
    "    \"N\": wordnet.NOUN,\n",
    "    \"V\": wordnet.VERB,\n",
    "    \"R\": wordnet.ADV,\n",
    "  }\n",
    "\n",
    "  def lemmatize(text: str):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    lemmatized_tokens = []\n",
    "    for token, tag in pos_tags:\n",
    "      wordnet_tag = wordnet_pos_tag_map.get(tag[0].upper())\n",
    "      if wordnet_tag is None:\n",
    "        lemmatized_tokens.append(token)\n",
    "      else:\n",
    "        lemmatized_tokens.append(wordnet_lemmatizer.lemmatize(token, wordnet_tag))\n",
    "\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "  df['text'] = df['text'].apply(lemmatize)\n",
    "  return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the pipeline and exporting to csv to skip reprocessing of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('data/dataset_processed.csv'):\n",
    "  df = pd.read_csv('data/dataset_processed.csv')\n",
    "else:\n",
    "  df = (\n",
    "    df\n",
    "    .pipe(denoiser)\n",
    "    .pipe(stopwords_remover)\n",
    "    .pipe(null_content_observation_remover)\n",
    "    .pipe(lemmatizer))\n",
    "\n",
    "  df.to_csv('data/dataset_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rwanda set host headquarters united nation dev...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>suck since im focus nature aspect thing enviro...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>also relate art lot people dismay start art ki...</td>\n",
       "      <td>uncertainty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>social security constant political crisis dist...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>broken rib puncture lung lead collapse lung mu...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        label\n",
       "0  rwanda set host headquarters united nation dev...     positive\n",
       "1  suck since im focus nature aspect thing enviro...     negative\n",
       "2  also relate art lot people dismay start art ki...  uncertainty\n",
       "3  social security constant political crisis dist...     negative\n",
       "4  broken rib puncture lung lead collapse lung mu...     negative"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label_map = {\n",
    "  'negative': 0,\n",
    "  'positive': 1,\n",
    "  'uncertainty': 2,\n",
    "}\n",
    "\n",
    "df['target'] = df['label'].apply(lambda label: target_label_map.get(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rwanda set host headquarters united nation dev...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>suck since im focus nature aspect thing enviro...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>also relate art lot people dismay start art ki...</td>\n",
       "      <td>uncertainty</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>social security constant political crisis dist...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>broken rib puncture lung lead collapse lung mu...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        label  target\n",
       "0  rwanda set host headquarters united nation dev...     positive       1\n",
       "1  suck since im focus nature aspect thing enviro...     negative       0\n",
       "2  also relate art lot people dismay start art ki...  uncertainty       2\n",
       "3  social security constant political crisis dist...     negative       0\n",
       "4  broken rib puncture lung lead collapse lung mu...     negative       0"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['text'].to_list()\n",
    "y = df['target'].to_list()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "  x, y,\n",
    "  test_size=0.33,\n",
    "  random_state=42,\n",
    "  stratify=y, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the split details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split size details\n",
      "X-original size: 691248, X-train: 463136, X-test: 228112\n",
      "y-original size: 691248, y-train: 463136, y-test: 228112\n",
      "Split percentage\n",
      "Train: 67%\n",
      "Test: 33%\n"
     ]
    }
   ],
   "source": [
    "print(\"Split size details\")\n",
    "print(f\"X-original size: {len(x)}, X-train: {len(x_train)}, X-test: {len(x_test)}\")\n",
    "print(f\"y-original size: {len(y)}, y-train: {len(y_train)}, y-test: {len(y_test)}\")\n",
    "print(\"Split percentage\")\n",
    "print(f\"Train: {round(len(x_train)/len(x) * 100)}%\")\n",
    "print(f\"Test: {round(len(x_test)/len(x) * 100)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/media/cabrera/LF26/Development/Python/anonalyze-process/models/sentiment-analysis/model-training-process.ipynb Cell 45\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/cabrera/LF26/Development/Python/anonalyze-process/models/sentiment-analysis/model-training-process.ipynb#Y126sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_naive_bayes \u001b[39m=\u001b[39m Pipeline([\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/cabrera/LF26/Development/Python/anonalyze-process/models/sentiment-analysis/model-training-process.ipynb#Y126sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     (\u001b[39m'\u001b[39;49m\u001b[39mtransformer\u001b[39;49m\u001b[39m'\u001b[39;49m, TfidfVectorizer()),\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/cabrera/LF26/Development/Python/anonalyze-process/models/sentiment-analysis/model-training-process.ipynb#Y126sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     (\u001b[39m'\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m'\u001b[39;49m, MultinomialNB(alpha\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)),\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/cabrera/LF26/Development/Python/anonalyze-process/models/sentiment-analysis/model-training-process.ipynb#Y126sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m ])\u001b[39m.\u001b[39;49mfit(x_train, y_train)\n",
      "File \u001b[0;32m/media/cabrera/LF26/Development/Python/anonalyze-process/.venv/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/media/cabrera/LF26/Development/Python/anonalyze-process/.venv/lib/python3.11/site-packages/sklearn/pipeline.py:469\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \n\u001b[1;32m    428\u001b[0m \u001b[39mFit all the transformers one after the other and sequentially transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    468\u001b[0m routed_params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_method_params(method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m\"\u001b[39m, props\u001b[39m=\u001b[39mparams)\n\u001b[0;32m--> 469\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, routed_params)\n\u001b[1;32m    470\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[1;32m    471\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m/media/cabrera/LF26/Development/Python/anonalyze-process/.venv/lib/python3.11/site-packages/sklearn/pipeline.py:406\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, routed_params)\u001b[0m\n\u001b[1;32m    404\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[1;32m    405\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[1;32m    407\u001b[0m     cloned_transformer,\n\u001b[1;32m    408\u001b[0m     X,\n\u001b[1;32m    409\u001b[0m     y,\n\u001b[1;32m    410\u001b[0m     \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    411\u001b[0m     message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPipeline\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    412\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(step_idx),\n\u001b[1;32m    413\u001b[0m     params\u001b[39m=\u001b[39;49mrouted_params[name],\n\u001b[1;32m    414\u001b[0m )\n\u001b[1;32m    415\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m/media/cabrera/LF26/Development/Python/anonalyze-process/.venv/lib/python3.11/site-packages/joblib/memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 312\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/media/cabrera/LF26/Development/Python/anonalyze-process/.venv/lib/python3.11/site-packages/sklearn/pipeline.py:1310\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m   1309\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1310\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mfit_transform(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mfit_transform\u001b[39;49m\u001b[39m\"\u001b[39;49m, {}))\n\u001b[1;32m   1311\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1312\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m\"\u001b[39m, {}))\u001b[39m.\u001b[39mtransform(\n\u001b[1;32m   1313\u001b[0m             X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtransform\u001b[39m\u001b[39m\"\u001b[39m, {})\n\u001b[1;32m   1314\u001b[0m         )\n",
      "File \u001b[0;32m/media/cabrera/LF26/Development/Python/anonalyze-process/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2091\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2084\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[1;32m   2085\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2086\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[1;32m   2087\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[1;32m   2088\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[1;32m   2089\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[1;32m   2090\u001b[0m )\n\u001b[0;32m-> 2091\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[1;32m   2092\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[1;32m   2093\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2094\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m/media/cabrera/LF26/Development/Python/anonalyze-process/.venv/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/media/cabrera/LF26/Development/Python/anonalyze-process/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1364\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1365\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1366\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1367\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1368\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1369\u001b[0m             )\n\u001b[1;32m   1370\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1372\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1374\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1375\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/media/cabrera/LF26/Development/Python/anonalyze-process/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1259\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1258\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1259\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1260\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1261\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m/media/cabrera/LF26/Development/Python/anonalyze-process/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:103\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[39ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[39mif\u001b[39;00m decoder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m     doc \u001b[39m=\u001b[39m decoder(doc)\n\u001b[1;32m    104\u001b[0m \u001b[39mif\u001b[39;00m analyzer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     doc \u001b[39m=\u001b[39m analyzer(doc)\n",
      "File \u001b[0;32m/media/cabrera/LF26/Development/Python/anonalyze-process/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:236\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    233\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39mdecode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode_error)\n\u001b[1;32m    235\u001b[0m \u001b[39mif\u001b[39;00m doc \u001b[39mis\u001b[39;00m np\u001b[39m.\u001b[39mnan:\n\u001b[0;32m--> 236\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    240\u001b[0m \u001b[39mreturn\u001b[39;00m doc\n",
      "\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "model_naive_bayes = Pipeline([\n",
    "    ('transformer', TfidfVectorizer()),\n",
    "    ('model', MultinomialNB(alpha=1)),\n",
    "]).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_value</th>\n",
       "      <th>predicted_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228107</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228108</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228109</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228110</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228111</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228112 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        actual_value  predicted_value\n",
       "0                  0                1\n",
       "1                  0                0\n",
       "2                  2                0\n",
       "3                  2                2\n",
       "4                  0                0\n",
       "...              ...              ...\n",
       "228107             1                2\n",
       "228108             1                1\n",
       "228109             2                2\n",
       "228110             2                2\n",
       "228111             0                0\n",
       "\n",
       "[228112 rows x 2 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_naive_bayes.predict(x_test)\n",
    "\n",
    "df_results = pd.DataFrame({\n",
    "  'actual_value': y_test,\n",
    "  'predicted_value': y_pred\n",
    "})\n",
    "\n",
    "df_results['classification'] = df_results.apply(\n",
    "  lambda x:\n",
    "    'true positive' if x['actual_value'] == 1 and x['predicted_value'] == 1 else\n",
    "    'true negative' if x['actual_value'] == 0 and x['predicted_value'] == 0 else\n",
    "    'true neutral' if x['actual_value'] == 2 and x['predicted_value'] == 2 else\n",
    "    'false positive' if x['actual_value'] != 1 and x['predicted_value'] == 1 else\n",
    "    'false negative' if x['actual_value'] != 0 and x['predicted_value'] == 0 else\n",
    "    'false neutral' if x['actual_value'] != 2 and x['predicted_value'] == 2 else\n",
    "    None\n",
    "  , axis=1\n",
    ")\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[73708,  4858,  2002],\n",
       "       [ 9041, 71078,  1891],\n",
       "       [10637,  7030, 47867]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(\n",
    "  y_true = y_test,\n",
    "  y_pred = model_naive_bayes.predict(x_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
