{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anonalyze: An NLP-Enhanced and ML-Driven Platform for Sentiment and Insight Extraction\n",
    "A platform designed like an online discussion board where users can freely share their thoughts and opinions anonymously. It uses AI, ML, and language processing tools to analyze the posts, helping to understand the overall mood and key ideas in the discussions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from nltk import pos_tag, sent_tokenize, RegexpParser\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.tree import Tree\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cabrera/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/cabrera/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/cabrera/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/cabrera/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/cabrera/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/cabrera/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/cabrera/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/cabrera/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/cabrera/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     /home/cabrera/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to /home/cabrera/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets_json to\n",
      "[nltk_data]     /home/cabrera/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping help/tagsets_json.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('tagsets_json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the pickled vectorizer, selector, and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/sentiment-emotion-classification/pkl/tfidf_vectorizer.pkl', 'rb') as file:\n",
    "  vectorizer = pickle.load(file)\n",
    "  \n",
    "with open('./models/sentiment-emotion-classification/pkl/selector_sentiment.pkl', 'rb') as file:\n",
    "  selector_sentiment = pickle.load(file)\n",
    "  \n",
    "with open('./models/sentiment-emotion-classification/pkl/selector_emotion.pkl', 'rb') as file:\n",
    "  selector_emotion = pickle.load(file)\n",
    "  \n",
    "with open('./models/sentiment-emotion-classification/pkl/model_sentiment.pkl', 'rb') as file:\n",
    "  model_sentiment = pickle.load(file)\n",
    "  \n",
    "with open('./models/sentiment-emotion-classification/pkl/model_emotion.pkl', 'rb') as file:\n",
    "  model_emotion = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a data pre-processor pipe composing of:\n",
    "* Denoising: removing the non-alphabetical characters in the content\n",
    "* Removing stopwords: removing stopwords such as `[a, an, the, and, but]`\n",
    "* Lemmatizing: reducing words to their base form e.g. `[changing, changed, change] -> change`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "  @staticmethod\n",
    "  def denoiser(text: str) -> str:\n",
    "    text = re.sub(r'@\\w+', '', text) \n",
    "    text = re.sub(r'[^a-zA-Z ]', '', text)\n",
    "    text = re.sub(r'https\\w+', '', text)\n",
    "    text = re.sub(r'http\\w+', '', text)\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "  @staticmethod\n",
    "  def stopwords_remover(text: str) -> str:\n",
    "    matcher = re.compile(r\"|\".join([fr\"\\b{word}\\b\" for word in stopwords.words(\"english\")]))\n",
    "    text = \" \".join(matcher.sub('', text).split())\n",
    "    return text\n",
    "\n",
    "  @staticmethod\n",
    "  def lemmatizer(text: str) -> str:\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "\n",
    "    wordnet_pos_tag_map = {\n",
    "        \"J\": wordnet.ADJ,\n",
    "        \"N\": wordnet.NOUN,\n",
    "        \"V\": wordnet.VERB,\n",
    "        \"R\": wordnet.ADV,\n",
    "    }\n",
    "\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    lemmatized_tokens = []\n",
    "    for token, tag in pos_tags:\n",
    "        wordnet_tag = wordnet_pos_tag_map.get(tag[0].upper())\n",
    "        if wordnet_tag is None:\n",
    "            lemmatized_tokens.append(token)\n",
    "        else:\n",
    "            lemmatized_tokens.append(wordnet_lemmatizer.lemmatize(token, wordnet_tag))\n",
    "            \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "  \n",
    "  @staticmethod\n",
    "  def process_text(text: str) -> str:\n",
    "    text = Preprocessor.denoiser(text)\n",
    "    text = Preprocessor.stopwords_remover(text)\n",
    "    text = Preprocessor.lemmatizer(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a static class for the sentiment model to simplify the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSentiment:\n",
    "  vectorizer = None\n",
    "  selector_sentiment = None\n",
    "  model_sentiment = None\n",
    "  \n",
    "  sentiment_label_description_map = {\n",
    "    0: 'negative',\n",
    "    1: 'positive',\n",
    "    2: 'neutral',\n",
    "  }\n",
    "\n",
    "  @staticmethod\n",
    "  def _initialize():\n",
    "    if ModelSentiment.vectorizer is None:\n",
    "      with open('./models/sentiment-emotion-classification/pkl/tfidf_vectorizer.pkl', 'rb') as file:\n",
    "          ModelSentiment.vectorizer = pickle.load(file)\n",
    "    \n",
    "    if ModelSentiment.selector_sentiment is None:\n",
    "      with open('./models/sentiment-emotion-classification/pkl/selector_sentiment.pkl', 'rb') as file:\n",
    "          ModelSentiment.selector_sentiment = pickle.load(file)\n",
    "    \n",
    "    if ModelSentiment.model_sentiment is None:\n",
    "      with open('./models/sentiment-emotion-classification/pkl/model_sentiment.pkl', 'rb') as file:\n",
    "          ModelSentiment.model_sentiment = pickle.load(file)\n",
    "  \n",
    "  @staticmethod\n",
    "  def _vectorize(text: str):\n",
    "    ModelSentiment._initialize()\n",
    "    return ModelSentiment.vectorizer.transform([text])\n",
    "\n",
    "  @staticmethod\n",
    "  def _select_best_features(vector):\n",
    "    return ModelSentiment.selector_sentiment.transform(vector)\n",
    "\n",
    "  @staticmethod\n",
    "  def predict(text: str):\n",
    "    ModelSentiment._initialize()\n",
    "    vector = ModelSentiment._vectorize(text)\n",
    "    vector = ModelSentiment._select_best_features(vector)\n",
    "    return (\n",
    "      ModelSentiment\n",
    "      .sentiment_label_description_map\n",
    "      .get(ModelSentiment.model_sentiment.predict(vector)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a static class for the emotion model to simplify the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEmotion:\n",
    "  vectorizer = None\n",
    "  selector_emotion = None\n",
    "  model_emotion = None\n",
    "  \n",
    "  emotion_label_description_map = {\n",
    "    0: 'sadness',\n",
    "    1: 'joy',\n",
    "    2: 'love',\n",
    "    3: 'anger',\n",
    "    4: 'fear',\n",
    "    5: 'surprised',\n",
    "  }\n",
    "\n",
    "  @staticmethod\n",
    "  def _initialize():\n",
    "    if ModelEmotion.vectorizer is None:\n",
    "      with open('./models/sentiment-emotion-classification/pkl/tfidf_vectorizer.pkl', 'rb') as file:\n",
    "          ModelEmotion.vectorizer = pickle.load(file)\n",
    "    \n",
    "    if ModelEmotion.selector_emotion is None:\n",
    "      with open('./models/sentiment-emotion-classification/pkl/selector_emotion.pkl', 'rb') as file:\n",
    "          ModelEmotion.selector_emotion = pickle.load(file)\n",
    "    \n",
    "    if ModelEmotion.model_emotion is None:\n",
    "      with open('./models/sentiment-emotion-classification/pkl/model_emotion.pkl', 'rb') as file:\n",
    "          ModelEmotion.model_emotion = pickle.load(file)\n",
    "  \n",
    "  @staticmethod\n",
    "  def _vectorize(text: str):\n",
    "    ModelEmotion._initialize()\n",
    "    return ModelEmotion.vectorizer.transform([text])\n",
    "\n",
    "  @staticmethod\n",
    "  def _select_best_features(vector):\n",
    "    return ModelEmotion.selector_emotion.transform(vector)\n",
    "\n",
    "  @staticmethod\n",
    "  def predict(text: str):\n",
    "    ModelEmotion._initialize()\n",
    "    vector = ModelEmotion._vectorize(text)\n",
    "    vector = ModelEmotion._select_best_features(vector)\n",
    "    \n",
    "    return (\n",
    "      ModelEmotion\n",
    "      .emotion_label_description_map\n",
    "      .get(ModelEmotion.model_emotion.predict(vector)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating the platform\n",
    "**Thread question**: How do you think about the impact of online anonymity on user behavior in social media platforms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = [\n",
    "  \"I believe online anonymity encourages more honest and open communication, allowing users to express their true opinions\",\n",
    "  \"In my view, online anonymity can lead to a significant increase in negative behaviors, such as trolling and cyberbullying, because users feel shielded from accountability.\",\n",
    "  \"I think anonymity provides a double-edged sword; while it allows for free expression, it also creates an environment where people may engage in harmful or deceitful actions.\",\n",
    "  \"Online anonymity empowers marginalized voices to speak out, but it also makes it difficult to identify and address harmful content effectively.\",\n",
    "  \"I see online anonymity as a critical factor in fostering diverse discussions, but it also contributes to the spread of misinformation, as sources cannot always be verified.\",\n",
    "  \"I think that online anonymity can lead to more genuine interactions in certain communities, but it may also reduce the quality of discourse by enabling users to avoid responsibility for their words.\",\n",
    "  \"Anonymity online is essential for privacy, but it can also encourage users to engage in behavior they might avoid if their identity were known.\",\n",
    "  \"In my opinion, the impact of online anonymity is largely context-dependent; it can promote both positive and negative behaviors depending on the platform and community norms.\",\n",
    "  \"I believe online anonymity amplifies both the best and worst aspects of human behavior, providing a space for both creativity and cruelty.\",\n",
    "  \"I think online anonymity allows people to connect more authentically, but it can also lead to a lack of trust and credibility in online interactions.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe online anonymity encourages more honest and open communication, allowing users t... \tsentiment: positive \temotion: joy\n",
      "In my view, online anonymity can lead to a significant increase in negative behaviors, suc... \tsentiment: positive \temotion: joy\n",
      "I think anonymity provides a double-edged sword; while it allows for free expression, it a... \tsentiment: positive \temotion: joy\n",
      "Online anonymity empowers marginalized voices to speak out, but it also makes it difficult... \tsentiment: positive \temotion: joy\n",
      "I see online anonymity as a critical factor in fostering diverse discussions, but it also ... \tsentiment: negative \temotion: joy\n",
      "I think that online anonymity can lead to more genuine interactions in certain communities... \tsentiment: positive \temotion: joy\n",
      "Anonymity online is essential for privacy, but it can also encourage users to engage in be... \tsentiment: positive \temotion: joy\n",
      "In my opinion, the impact of online anonymity is largely context-dependent; it can promote... \tsentiment: positive \temotion: joy\n",
      "I believe online anonymity amplifies both the best and worst aspects of human behavior, pr... \tsentiment: negative \temotion: sadness\n",
      "I think online anonymity allows people to connect more authentically, but it can also lead... \tsentiment: negative \temotion: joy\n"
     ]
    }
   ],
   "source": [
    "for response in responses:\n",
    "  preprocessed_text = Preprocessor.process_text(response)\n",
    "  predicted_sentiment = ModelSentiment.predict(preprocessed_text)\n",
    "  predicted_emotion = ModelEmotion.predict(preprocessed_text)\n",
    "  print(f\"{response[:90]}... \\tsentiment: {predicted_sentiment} \\temotion: {predicted_emotion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subject-Verb-Object Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t\t\t[PRP] pronoun, personal\n",
      "believe \t\t\t[VBP] verb, present tense, not 3rd person singular\n",
      "online \t\t\t[JJ] adjective or numeral, ordinal\n",
      "anonymity \t\t\t[NN] noun, common, singular or mass\n",
      "encourages \t\t\t[VBZ] verb, present tense, 3rd person singular\n",
      "more \t\t\t[JJR] adjective, comparative\n",
      "honest \t\t\t[JJ] adjective or numeral, ordinal\n",
      "and \t\t\t[CC] conjunction, coordinating\n",
      "open \t\t\t[JJ] adjective or numeral, ordinal\n",
      "communication \t\t\t[NN] noun, common, singular or mass\n",
      ", \t\t\t[,] comma\n",
      "allowing \t\t\t[VBG] verb, present participle or gerund\n",
      "users \t\t\t[NNS] noun, common, plural\n",
      "to \t\t\t[TO] \"to\" as preposition or infinitive marker\n",
      "express \t\t\t[VB] verb, base form\n",
      "their \t\t\t[PRP$] pronoun, possessive\n",
      "true \t\t\t[JJ] adjective or numeral, ordinal\n",
      "opinions \t\t\t[NNS] noun, common, plural\n"
     ]
    }
   ],
   "source": [
    "tagdict = nltk.data.load('help/tagsets/upenn_tagset.pickle')\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "tokens = tokenizer.tokenize(responses[0])\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "for tagged_token in tagged_tokens:\n",
    "  print(f\"{tagged_token[0]} \\t\\t\\t[{tagged_token[1]}] {tagdict[tagged_token[1]][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anonymity encourages communication\n",
      "communication allowing users\n",
      "users express opinions\n",
      "anonymity lead increase\n",
      "users feel accountability\n",
      "users shielded accountability\n",
      "anonymity provides sword\n",
      "anonymity edged sword\n",
      "sword allows expression\n",
      "expression creates environment\n",
      "people engage harmful\n",
      "empowers marginalized voices\n",
      "voices speak content\n",
      "voices makes content\n",
      "voices identify content\n",
      "voices address content\n",
      "factor fostering discussions\n",
      "discussions contributes spread\n",
      "anonymity lead interactions\n",
      "communities reduce quality\n",
      "discourse enabling users\n",
      "users avoid responsibility\n",
      "online is privacy\n",
      "privacy encourage users\n",
      "users engage behavior\n",
      "behavior avoid identity\n",
      "anonymity is dependent\n",
      "dependent promote behaviors\n",
      "behaviors depending platform\n",
      "behavior providing space\n",
      "anonymity allows people\n",
      "people connect lack\n",
      "people lead lack\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "for response in responses:\n",
    "  tokens = tokenizer.tokenize(response)\n",
    "  tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "  for i, (token, tag) in enumerate(tagged_tokens):\n",
    "    \n",
    "    # Verb might indicate relationship\n",
    "    if tag.startswith('VB'):\n",
    "      verb = token\n",
    "      subj = None\n",
    "      obj = None\n",
    "      \n",
    "      # Look for the nearest noun or pronoun before the verb\n",
    "      for j in range(i-1, -1, -1):\n",
    "        if tagged_tokens[j][1].startswith('NN'):\n",
    "          subj = tagged_tokens[j][0]\n",
    "          break\n",
    "        \n",
    "      # Look for the nearest noun or pronoun after the verb\n",
    "      for j in range(i+1, len(tagged_tokens)):\n",
    "        if tagged_tokens[j][1].startswith('NN'):\n",
    "          obj = tagged_tokens[j][0]\n",
    "          break\n",
    "      \n",
    "      if subj and obj:\n",
    "        print(f\"{subj} {verb} {obj}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online anonymity empowers marginalized voices\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "for response in responses:\n",
    "  tokens = tokenizer.tokenize(response)\n",
    "  tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "  grammar = r\"\"\"\n",
    "    NP: {<DT>?<JJ.*>*<NN.*>+}\n",
    "    VP: {<VB.*><NP|PP|CLAUSE>+}\n",
    "    CLAUSE: {<NP><VP>}\n",
    "  \"\"\"\n",
    "\n",
    "  chunker = RegexpParser(grammar)\n",
    "  chunked = chunker.parse(tagged_tokens)\n",
    "  \n",
    "  for tagged_token_group in chunked:\n",
    "    if type(tagged_token_group) == nltk.Tree:\n",
    "      subtree = tagged_token_group\n",
    "      if subtree.label() == \"CLAUSE\":\n",
    "        print(\" \".join(word for word,tag in subtree.leaves()))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open communication allowing users\n",
      "free expression creates an environment\n",
      "Online anonymity empowers marginalized voices\n",
      "Online anonymity empowers address harmful content\n",
      "a critical factor fostering diverse discussions\n",
      "certain communities reduce the quality\n",
      "discourse enabling users\n",
      "discourse avoid responsibility\n",
      "privacy encourage users\n",
      "human behavior providing a space\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "for response in responses:\n",
    "  tokens = tokenizer.tokenize(response)\n",
    "  tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "  grammar = r\"\"\"\n",
    "    NP: {<DT>?<JJ.*>*<NN.*>+}\n",
    "    VP: {<VB.*><NP|PP|CLAUSE>+}\n",
    "  \"\"\"\n",
    "\n",
    "  chunker = RegexpParser(grammar)\n",
    "  chunked = chunker.parse(tagged_tokens)\n",
    "  \n",
    "  for i, tagged_token in enumerate(chunked):\n",
    "    if type(tagged_token) == nltk.Tree:\n",
    "      subtree = tagged_token\n",
    "      \n",
    "      if subtree.label() == 'VP':\n",
    "        verb_phrase = None\n",
    "        start_noun_phrase = None\n",
    "        \n",
    "        verb_phrase = \" \".join([word for word,tag in subtree.leaves()])\n",
    "        \n",
    "        # Look for the nearest noun phrase before verb phrase \n",
    "        for j in range(i-1, -1, -1):\n",
    "          tagged_token_group = chunked[j]\n",
    "          if type(tagged_token_group) == nltk.Tree and tagged_token_group.label() == 'NP':\n",
    "            start_noun_phrase = \" \".join([word for word,tag in tagged_token_group.leaves()])\n",
    "            break\n",
    "          \n",
    "        if start_noun_phrase and verb_phrase:\n",
    "          print(f\"{start_noun_phrase} {verb_phrase}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
